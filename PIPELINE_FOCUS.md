# Sales Pipeline - Focus Guide

## ğŸ¯ Dashboard Overview

### What is the Flask Dashboard?
Your **Flask web application** displays real-time sales analytics by querying the **Silver layer** Delta tables.

**Location**: [app/web/app.py](app/web/app.py)  
**Port**: 5000  
**URL**: http://localhost:5000

### Dashboard Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    SALES ANALYTICS DASHBOARD        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“Š Aggregated Sales                â”‚
â”‚  â”œâ”€ Total sales by product          â”‚
â”‚  â”œâ”€ Sales by country                â”‚
â”‚  â””â”€ Revenue metrics                 â”‚
â”‚                                      â”‚
â”‚  ğŸ† Top Products                    â”‚
â”‚  â”œâ”€ Top 10 products by revenue      â”‚
â”‚  â””â”€ Product performance ranking     â”‚
â”‚                                      â”‚
â”‚  â° Hourly Sales Trends             â”‚
â”‚  â”œâ”€ Sales by hour                   â”‚
â”‚  â””â”€ Time-series visualization       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Endpoints

The dashboard exposes 4 main APIs:

```
1. GET /api/aggregated_sales
   â””â”€ Returns: [{"produit_nom", "pays", "total_montant", "total_quantite"}]
   â””â”€ Source: /delta/silver/ventes_aggreges

2. GET /api/top_products
   â””â”€ Returns: [{"produit_nom", "total_montant", "rank"}]
   â””â”€ Source: /delta/silver/top_produits

3. GET /api/hourly_sales
   â””â”€ Returns: [{"heure", "total_montant", "total_quantite"}]
   â””â”€ Source: /delta/silver/hourly_sales

4. GET /api/product_by_segment
   â””â”€ Returns: [{"produit", "segment", "montant"}]
   â””â”€ Source: /delta/silver/ventes_aggreges
```

### How It Works

```
User visits http://localhost:5000
        â†“
Flask loads HTML template (index.html)
        â†“
JavaScript fetches from /api/aggregated_sales
        â†“
Flask creates Spark session (lazy load)
        â†“
Spark reads Delta table: /delta/silver/ventes_aggreges
        â†“
Returns JSON response
        â†“
Dashboard displays charts & metrics
```

---

## ğŸ—ï¸ Why Bronze Layer?

### Purpose: **Store Raw, Unmodified Data**

The Bronze layer is your **single source of truth** - it preserves the original data exactly as it arrives.

### Key Reasons

| Reason | Benefit |
|--------|---------|
| **Immutability** | Original data never changes - provides audit trail |
| **Recovery** | If Silver transformations fail, you can reprocess from Bronze |
| **Schema Evolution** | Handle schema changes without losing historical data |
| **Compliance** | Keep raw data for legal/audit requirements |
| **Debugging** | Compare raw data vs transformed data to find bugs |

### Bronze Layer Structure

```
/delta/bronze/ventes_raw/
â”‚
â”œâ”€ date=2025-12-22/
â”‚  â”œâ”€ part-00000.parquet (messages from 00:00-01:00)
â”‚  â”œâ”€ part-00001.parquet (messages from 01:00-02:00)
â”‚  â””â”€ ...
â”‚
â””â”€ _delta_log/  (transaction history)
```

### Bronze Data Example

```json
{
  "vente_id": 1,
  "client_id": 1,
  "produit_id": 101,
  "timestamp": "2025-12-22T10:30:45.123456",
  "quantite": 2,
  "montant": 1799.98,
  "client_nom": "Jean Dupont",
  "produit_nom": "Ordinateur portable",
  "categorie": "Electronique",
  "pays": "France",
  "segment": "Particulier"
}
```

**No transformations applied** - exactly as received from Kafka!

### Bronze Processing Step

**File**: [dags/spark_streaming_delta.py](dags/spark_streaming_delta.py)

```python
# Minimal transformation - just add metadata
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_SERVERS) \
    .option("subscribe", "ventes_stream") \
    .load()

# Parse JSON + add ingestion timestamp
df_parsed = df.select(
    from_json(col("value").cast("string"), schema).alias("data"),
    current_timestamp().alias("ingestion_time")
)

# Write to Bronze (append mode - immutable)
df_parsed.write \
    .format("delta") \
    .mode("append") \
    .partitionBy("date") \
    .save("/delta/bronze/ventes_raw")
```

---

## ğŸ”„ Why Aggregations in Silver Layer?

### Purpose: **Transform Raw Data Into Business Insights**

The Silver layer **cleans, enriches, and aggregates** Bronze data into useful business metrics.

### Key Reasons

| Reason | Benefit |
|--------|---------|
| **Performance** | Pre-aggregated data = faster dashboard queries |
| **Quality** | Remove duplicates, handle nulls, validate |
| **Business Value** | Convert raw transactions into insights |
| **Denormalization** | Optimize for read patterns (dashboard queries) |
| **Separation of Concerns** | Raw data separate from analytics data |

### Silver Layer Structure

```
/delta/silver/
â”œâ”€ ventes_clean/          â† Cleaned transactions
â”œâ”€ ventes_aggreges/       â† GROUP BY product + country
â”œâ”€ top_produits/          â† Ranked products
â””â”€ hourly_sales/          â† GROUP BY hour
```

### Example: Aggregations in Silver

**Table 1: ventes_aggreges (Product Ã— Country)**

```
produit_nom              | pays     | total_montant | total_quantite
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ordinateur portable      | France   | 8,999.90      | 10
Ordinateur portable      | Spain    | 5,499.50      | 6
Souris sans fil          | France   | 765.00        | 30
Clavier mecanique        | UK       | 2,250.00      | 30
```

**Query Used**:
```python
ventes_aggreges = df_clean.groupBy(
    "produit_nom", "pays"
).agg(
    F.sum("montant").alias("total_montant"),
    F.sum("quantite").alias("total_quantite")
)
```

**Table 2: top_produits (Top 10 by Revenue)**

```
rank | produit_nom              | total_montant
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1    | Ordinateur portable      | 14,499.40
2    | Clavier mecanique        | 3,750.00
3    | Casque audio             | 2,999.50
4    | Souris sans fil          | 765.00
5    | Livre Data Science       | 399.80
```

**Table 3: hourly_sales (Time Series)**

```
heure              | total_montant | total_quantite
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2025-12-22 10:00   | 1,500.00      | 20
2025-12-22 11:00   | 2,250.00      | 30
2025-12-22 12:00   | 999.90        | 15
```

### Silver Processing Step

**File**: [dags/bronze_to_silver.py](dags/bronze_to_silver.py)

```python
# Step 1: Read from Bronze
df_bronze = spark.read.format("delta").load("/delta/bronze/ventes_raw")

# Step 2: Data Cleaning
df_clean = df_bronze \
    .dropDuplicates() \
    .filter(col("montant") >= 0) \  # Validation
    .filter(col("client_id").isNotNull()) \  # Quality check
    .withColumn("timestamp", to_timestamp("timestamp"))

# Step 3: Create Aggregations
ventes_aggreges = df_clean.groupBy(
    "produit_nom", "pays"
).agg(
    F.sum("montant").alias("total_montant"),
    F.sum("quantite").alias("total_quantite")
)

# Step 4: Write to Silver
ventes_aggreges.write \
    .format("delta") \
    .mode("overwrite") \  # Replace old aggregations
    .save("/delta/silver/ventes_aggreges")
```

---

## ğŸ“Š Complete Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA GENERATION   â”‚
â”‚  50 fake sales      â”‚
â”‚  every run          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£  KAFKA INGESTION (Real-time Stream)      â”‚
â”‚                                              â”‚
â”‚ Topic: ventes_stream                         â”‚
â”‚ Brokers: broker1, broker2, broker3           â”‚
â”‚ Message format: JSON (client + product data) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£  AIRFLOW ORCHESTRATION                   â”‚
â”‚                                              â”‚
â”‚ DAG: unified_sales_pipeline                  â”‚
â”‚ â”œâ”€ check_infrastructure                     â”‚
â”‚ â”œâ”€ produce_sales_data â†’ Kafka               â”‚
â”‚ â”œâ”€ run_bronze_processing â†’ Spark            â”‚
â”‚ â”œâ”€ check_bronze_data_ready                  â”‚
â”‚ â”œâ”€ run_silver_processing â†’ Spark            â”‚
â”‚ â””â”€ verify_final_output                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£  SPARK BRONZE PROCESSING                â”‚
â”‚                                              â”‚
â”‚ Input: Kafka topic "ventes_stream"          â”‚
â”‚ Processing:                                 â”‚
â”‚ â”œâ”€ Connect to Kafka                         â”‚
â”‚ â”œâ”€ Parse JSON messages                      â”‚
â”‚ â”œâ”€ Add ingestion timestamp                  â”‚
â”‚ â””â”€ Handle streaming checkpoints             â”‚
â”‚                                              â”‚
â”‚ Output: /delta/bronze/ventes_raw            â”‚
â”‚ (Raw, unmodified data)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4ï¸âƒ£  SPARK SILVER PROCESSING                â”‚
â”‚                                              â”‚
â”‚ Input: /delta/bronze/ventes_raw             â”‚
â”‚ Transformations:                            â”‚
â”‚ â”œâ”€ Remove duplicates                        â”‚
â”‚ â”œâ”€ Validate data (montant >= 0)             â”‚
â”‚ â”œâ”€ Standardize timestamps                   â”‚
â”‚ â”œâ”€ GROUP BY product + country               â”‚
â”‚ â”œâ”€ GROUP BY hour                            â”‚
â”‚ â””â”€ Rank products by revenue                 â”‚
â”‚                                              â”‚
â”‚ Output: /delta/silver/                      â”‚
â”‚ â”œâ”€ ventes_clean (cleaned transactions)      â”‚
â”‚ â”œâ”€ ventes_aggreges (product Ã— country)      â”‚
â”‚ â”œâ”€ top_produits (top 10 products)           â”‚
â”‚ â””â”€ hourly_sales (hourly aggregates)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5ï¸âƒ£  FLASK DASHBOARD                        â”‚
â”‚                                              â”‚
â”‚ APIs read from Silver layer:                â”‚
â”‚ â”œâ”€ /api/aggregated_sales â†’ ventes_aggreges â”‚
â”‚ â”œâ”€ /api/top_products â†’ top_produits         â”‚
â”‚ â””â”€ /api/hourly_sales â†’ hourly_sales         â”‚
â”‚                                              â”‚
â”‚ Port: 5000                                  â”‚
â”‚ URL: http://localhost:5000                  â”‚
â”‚                                              â”‚
â”‚ Displays:                                   â”‚
â”‚ â”œâ”€ Sales metrics by product/country         â”‚
â”‚ â”œâ”€ Top 10 products ranking                  â”‚
â”‚ â””â”€ Time-series hourly trends                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## â±ï¸ Pipeline Timing

```
Step 1: Check Infrastructure        â†’ 5 seconds
Step 2: Produce 50 Sales to Kafka   â†’ 10 seconds
Step 3: Check Data Ready            â†’ 5 seconds
Step 4: Bronze Processing           â†’ 30-60 seconds (Spark job)
Step 5: Silver Processing           â†’ 30-60 seconds (Aggregations)
Step 6: Verify Output               â†’ 5 seconds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Duration                      â†’ 1.5 - 2.5 minutes
```

---

## ğŸ” Key Differences: Bronze vs Silver

| Aspect | Bronze | Silver |
|--------|--------|--------|
| **Data** | Raw, unchanged | Cleaned, aggregated |
| **Source** | Kafka stream | Bronze layer |
| **Write Mode** | Append (immutable) | Overwrite (updated) |
| **Use Case** | Audit trail, recovery | Analytics, reporting |
| **Query Performance** | Slower (raw data) | Fast (pre-aggregated) |
| **Schema** | Schema may evolve | Fixed, validated schema |
| **Data Volume** | Large (all raw records) | Small (aggregated metrics) |

---

## ğŸ“ˆ Why This Architecture?

```
Kafka (streaming)
    â†“
    â””â”€â†’ Bronze (store everything)
         â””â”€â†’ Silver (compute aggregations)
             â””â”€â†’ Dashboard (serve analytics)
```

**Benefits**:
1. âœ… **Scalability**: Kafka handles high-volume streaming
2. âœ… **Reliability**: Bronze layer is immutable backup
3. âœ… **Performance**: Silver aggregations are pre-computed
4. âœ… **Flexibility**: Can reprocess Bronze if needed
5. âœ… **Auditability**: Full data lineage preserved

---

## ğŸš€ Running Just the Pipeline

```bash
# Start all services
docker-compose up --build -d

# View Airflow logs
docker-compose logs -f airflow-scheduler

# Trigger pipeline manually
docker-compose exec airflow-scheduler airflow dags trigger unified_sales_pipeline

# Monitor Spark job
docker-compose logs -f spark-master

# Access dashboard
# Open browser: http://localhost:5000
```

---

## ğŸ“Š Expected Dashboard Output

After running the pipeline, you'll see:

**Aggregated Sales** (by product & country):
```
Ordinateur portable    | France      | â‚¬8,999.90   | 10 units
Souris sans fil        | France      | â‚¬765.00     | 30 units
Clavier mecanique      | UK          | â‚¬2,250.00   | 30 units
...
```

**Top Products**:
```
1. Ordinateur portable  | â‚¬14,499.40
2. Clavier mecanique    | â‚¬3,750.00
3. Casque audio         | â‚¬2,999.50
```

**Hourly Sales**:
```
2025-12-22 10:00 | â‚¬1,500.00 | 20 units
2025-12-22 11:00 | â‚¬2,250.00 | 30 units
2025-12-22 12:00 | â‚¬999.90   | 15 units
```

---

## ğŸ¯ Summary

| Component | Purpose | Output |
|-----------|---------|--------|
| **Kafka** | Real-time data ingestion | Stream of sales events |
| **Bronze** | Raw data storage | /delta/bronze/ventes_raw |
| **Silver** | Data transformation & aggregation | /delta/silver/* (analytics-ready) |
| **Dashboard** | Visualization & reporting | Web UI at port 5000 |

**The magic**: Bronze stores everything, Silver computes useful metrics, Dashboard displays them in real-time! ğŸ‰
